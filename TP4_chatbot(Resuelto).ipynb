{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bustagar/PROCESAMIENTO-DE-HABLA/blob/main/TP4_chatbot(Resuelto).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP4 Chatbot\n",
        "\n",
        "## Nombre: Juan Sebastian Bustamante Garcia"
      ],
      "metadata": {
        "id": "BDydrx9x1PF8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hghx5BwKdm-A",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install spacy --quiet\n",
        "!python -m spacy download es_core_news_sm --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "doc = nlp(\"Esto es una frase.\")\n",
        "print([(w.text, w.pos_) for w in doc])"
      ],
      "metadata": {
        "id": "D9SRh-eDdpWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qr3pyit4kWQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbots basados en recuperación\n",
        "\n",
        "En inglés information retrieval chatbots"
      ],
      "metadata": {
        "id": "UnrG-1ylkncZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motor de búsqueda"
      ],
      "metadata": {
        "id": "s3bs1FmWkfrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Búsqueda por palabras clave: Extrae palabras clave de la pregunta del usuario y busca coincidencias en las preguntas almacenadas.\n",
        "\n",
        "* Similitud del coseno: Si has representado las preguntas como vectores (por ejemplo, usando TF-IDF o word embeddings), puedes usar la similitud del coseno para medir la distancia entre las preguntas.\n",
        "\n",
        "* Word embeddings: Utiliza modelos de word embeddings como Word2Vec o BERT para obtener representaciones semánticas de las preguntas y las consultas del usuario."
      ],
      "metadata": {
        "id": "enbz9kXGkWlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Búsqueda por palabras claves"
      ],
      "metadata": {
        "id": "UQoVRp4gjxUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tu_diccionario = {\n",
        "   \"hola\": \"¡Hola! ¿En qué puedo ayudarte?\",\n",
        "   \"adiós\": \"Hasta luego. ¡Que tengas un buen día!\",\n",
        "   \"información\": \"¿Qué tipo de información estás buscando?\",\n",
        "   # Agrega más entradas de diccionario según tus necesidades\n",
        "}\n"
      ],
      "metadata": {
        "id": "3BljEMEOhpTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_pregunta(pregunta):\n",
        "    pregunta_procesada = nlp(pregunta.lower())  # Procesa la pregunta y convierte a minúsculas\n",
        "    respuesta = \"Lo siento, no entiendo tu pregunta.\"\n",
        "\n",
        "    # Busca una coincidencia en el diccionario\n",
        "    for palabra in pregunta_procesada:\n",
        "        # regresa la primer coincidencia que encuentra\n",
        "        if palabra.text in tu_diccionario:\n",
        "            respuesta = tu_diccionario[palabra.text]\n",
        "            break\n",
        "\n",
        "    return respuesta\n"
      ],
      "metadata": {
        "id": "Z4q_k1_3hvyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    entrada_usuario = input(\"Tú: \")\n",
        "    if entrada_usuario.lower() == \"salir\":\n",
        "        print(\"Chatbot: Hasta luego.\")\n",
        "        break\n",
        "    respuesta = responder_pregunta(entrada_usuario)\n",
        "    print(\"Chatbot:\", respuesta)\n"
      ],
      "metadata": {
        "id": "9gMPKi-ghwcA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6S88twrY5sOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Búsqueda por similitud"
      ],
      "metadata": {
        "id": "nI2FsyUOj3je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para los chatbots basados ​​en recuperación, es común utilizar bolsas de palabras (bag of words) o tf-idf para calcular la similitud de intenciones."
      ],
      "metadata": {
        "id": "AoZIaX0Kj7xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Datos de ejemplo\n",
        "preguntas = [\"¿Qué es el aprendizaje automático?\",\n",
        "             \"¿Cómo funciona la regresión lineal?\"]\n",
        "respuestas = [\"El aprendizaje automático es una rama de la inteligencia artificial...\",\n",
        "              \"La regresión lineal es un método de modelado...\"]\n",
        "\n",
        "# Vectorización con TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preguntas)\n",
        "\n",
        "# Función para encontrar la mejor coincidencia\n",
        "def responder_pregunta(consulta_usuario):\n",
        "    consulta_vec = vectorizer.transform([consulta_usuario])\n",
        "    similitudes = cosine_similarity(consulta_vec, tfidf_matrix).flatten()\n",
        "    print(similitudes)\n",
        "    indice_mejor_coincidencia = similitudes.argmax()\n",
        "    print(indice_mejor_coincidencia)\n",
        "    return respuestas[indice_mejor_coincidencia]\n"
      ],
      "metadata": {
        "id": "CReIz0ISj75s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ejemplo de consulta\n",
        "consulta = \"¿Qué es la regresión lineal?\"\n",
        "print(responder_pregunta(consulta))\n"
      ],
      "metadata": {
        "id": "foqaZ1FN583i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Búsqueda por similitud en embeddings"
      ],
      "metadata": {
        "id": "20KCxfCymOHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedes vectorizar el texto usando embeddings, como vimos la clase pasada.\n"
      ],
      "metadata": {
        "id": "7g6jgLSLnilX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actividades\n",
        "\n",
        "### 1) Elaborar un dataset de preguntas y respuestas para crear un Chatbot para un aplicación particular. ( 3 puntos )\n",
        "\n",
        "1.1 Debe definir la aplicación (atención al cliente bancario, atención a estudiantes universitarios, etc).\n",
        "1.2 El listado de preguntas y respuestas debe tener como mínimo 20 elementos pregunta - respuesta.\n",
        "\n",
        "###  2) Crear el chatbot utilizando TFIDF y similitud del coseno. (1 punto)\n",
        "\n",
        "### 3) Crear otro chatbot utilizando embeddings. Indique cuál embedding (1 punto) pre-entrenado eligió.\n",
        "\n",
        "### 4) Muestra ambos chatbots funcionando (1 punto)\n",
        "\n",
        "Adjuntar la lista de preguntas utilizadas para probar el funcionamiento.\n",
        "\n",
        "### 5) Añade tus conclusiones de todo lo realizado (2 punto)\n",
        "\n",
        "### 6) BONUS: usa lo realizado en 1 y 3 para crear un chatbot RAG. (2 puntos)\n",
        "\n",
        "* Utiliza un modelo LLM pre-entrenado.\n",
        "\n",
        "* Este punto no es obligatorio de realizar para quienes quieran regularizar / recuperar y luego rendirán en mesa.\n",
        "* Para quienes tienen condiciones para promocionar (han realizado y entregado los TPs a tiempo) la resolución de este ejercicio será tenida en cuenta para sumar a la promoción.\n",
        "\n",
        "### 7) No olvides:\n",
        "\n",
        "* Explicar tus decisiones y configuraciones. Añadir tus conclusiones.\n",
        "* Anunciar en el foro cuál será tu aplicación y postear tu entrega y tus avances.\n",
        "* Debes subir tu notebook a un repo GitHub público de tu propiedad compartido + enlace colab.\n",
        "* Documentar todo el proceso.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1sGxF1VglJVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Elección de aplicación (Chatbot Jurídico)"
      ],
      "metadata": {
        "id": "o_6AVESuyxsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy --quiet\n",
        "!python -m spacy download es_core_news_sm --quiet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8wy87dxN434A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "bkw8I6I64Gxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga del dataset\n",
        "url = 'https://raw.githubusercontent.com/Bustagar/PROCESAMIENTO-DE-HABLA/refs/heads/main/Preguntas_Derecho.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "gcJawUbYy_ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga del modelo de spaCy y descarga de recursos\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "nltk.download('stopwords')\n",
        "stopwords_es = set(nltk.corpus.stopwords.words('spanish'))"
      ],
      "metadata": {
        "id": "T51nT_Rg-HGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Procesamiento y limpieza"
      ],
      "metadata": {
        "id": "ilzJvnb0-KWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_texto(text):\n",
        "    text = text.lower()                                    # Convertir todo a minúsculas\n",
        "    text = re.sub(r'[.¡!¿?\\'\\\"“”‘’«»…]', '', text)         # Eliminar signos de puntuación comunes\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()               # Reemplazar múltiples espacios por uno\n",
        "    text = re.sub(r'\\.\\.\\.', '', text)                     # Eliminar puntos suspensivos\n",
        "    text = re.sub(r'[^a-zñáéíóúü\\s]', '', text)            # Quitar todo lo que no sea letra o espacio\n",
        "    return text\n",
        "\n",
        "def lematizar_y_filtrar(text):\n",
        "    doc = nlp(text)                                        # Procesar el texto con spaCy\n",
        "    lemmas = [token.lemma_ for token in doc\n",
        "              if token.lemma_ not in stopwords_es and token.is_alpha]  # Lemas sin stopwords\n",
        "    return \" \".join(lemmas)                                # Unir los lemas en una sola cadena\n",
        "\n",
        "def procesar_preguntas(preguntas):\n",
        "    preguntas_limpias = [limpiar_texto(q) for q in preguntas]           # Aplicar limpieza básica a cada pregunta\n",
        "    preguntas_procesadas = [lematizar_y_filtrar(q) for q in preguntas_limpias]  # Lematizar y eliminar stopwords\n",
        "    return preguntas_procesadas"
      ],
      "metadata": {
        "id": "pexW_bIw-iUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Se lematizaron primeramente las preguntas antes de quitar las stopword ya que al haber muchas palabras conjugadas o flexionadas no las reconoce el stopword y las deja en el texto, produciendo mas ruido."
      ],
      "metadata": {
        "id": "yCEOLzFyCNoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Vectorizacion aplicando TF-IDF"
      ],
      "metadata": {
        "id": "TwWy529yDQDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preguntas_procesadas = procesar_preguntas(df['Pregunta'].tolist())  # Procesar todas las preguntas\n",
        "\n",
        "df['Pregunta_procesada'] = preguntas_procesadas           # Agregar columna con preguntas procesadas al DataFrame\n",
        "\n",
        "vectorizer = TfidfVectorizer()                            # Crear el vectorizador TF-IDF\n",
        "X = vectorizer.fit_transform(df['Pregunta_procesada'])    # Vectorizar las preguntas del corpus"
      ],
      "metadata": {
        "id": "QeUhaIGsCyP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Chatbot con TF-IDF"
      ],
      "metadata": {
        "id": "oN2P-NilEH22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_responder(pregunta_usuario):\n",
        "    entrada = pregunta_usuario.lower().strip()\n",
        "\n",
        "    # Respuesta personalizada si detecta un saludo simple\n",
        "    if entrada in ['hola', 'buenas', 'buen día', 'buenas tardes', 'buenas noches', 'qué tal', 'saludos']:\n",
        "        return \"Hola! ¿En qué puedo ayudarte?\"\n",
        "\n",
        "    pregunta_proc = lematizar_y_filtrar(limpiar_texto(entrada)) # Procesar la entrada del usuario\n",
        "    vector_usuario = vectorizer.transform([pregunta_proc])      # Vectorizar la pregunta procesada\n",
        "    similitudes = cosine_similarity(vector_usuario, X)          # Calcular similitud entre usuario y corpus\n",
        "\n",
        "    idx_max = similitudes.argmax()                   # Obtener el índice de la mejor coincidencia\n",
        "    score = similitudes[0][idx_max]                  # Obtener el puntaje de similitud más alto\n",
        "\n",
        "    if score > 0.3:                                  # Si el puntaje es razonable\n",
        "        return df.iloc[idx_max]['Respuesta']         # Devolver la respuesta correspondiente\n",
        "    else:\n",
        "        return \"No tengo una respuesta segura. ¿Podrías reformular la pregunta?\"  # Mensaje por defecto\n"
      ],
      "metadata": {
        "id": "LQfBWCikELzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación de la similitud del coseno"
      ],
      "metadata": {
        "id": "IjqIbjuLH4Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluar_similitud(pregunta_usuario):\n",
        "    # Preprocesar la entrada\n",
        "    pregunta_proc = lematizar_y_filtrar(limpiar_texto(pregunta_usuario))\n",
        "\n",
        "    # Vectorizar\n",
        "    pregunta_vec = vectorizer.transform([pregunta_proc])\n",
        "\n",
        "    # Calcular similitud con todas las preguntas del corpus\n",
        "    similitudes = cosine_similarity(pregunta_vec, X)\n",
        "\n",
        "    # Obtener índice y puntaje de la mejor coincidencia\n",
        "    idx_max = similitudes.argmax()\n",
        "    score_max = similitudes[0][idx_max]\n",
        "\n",
        "    # Respuesta asociada\n",
        "    respuesta = df.iloc[idx_max]['Respuesta']\n",
        "\n",
        "    # Imprimir detalle\n",
        "    print(f\"Pregunta procesada:  {pregunta_proc}\")\n",
        "    print(f\"Similitud máxima:    {score_max:.4f}\")\n",
        "    print(f\"Índice en dataset:   {idx_max}\")\n",
        "    print(f\"Respuesta sugerida:  {respuesta}\")\n",
        "\n",
        "    return score_max, idx_max, respuesta\n"
      ],
      "metadata": {
        "id": "WN-YIJqdH7Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se prueba la similitud\n",
        "pregunta = \"Delito doloso\"\n",
        "print(evaluar_similitud(pregunta))\n"
      ],
      "metadata": {
        "id": "kKeFPPS7I7uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se observa que al evaluar el indice del coseno se obtiene una similitud del 99% lo cual sugiere que el modelo capta muy bien la similitud"
      ],
      "metadata": {
        "id": "jgzpiudbJCYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Crear otro chatbot utilizando embeddings"
      ],
      "metadata": {
        "id": "cgHXncrcNA6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MiniLm: Se utilizó este modelo ya que es liviano y multilingual y muy eficiente para medir similitud semántica entre frases, lo cual encontré que es ideal para lenguaje Jurídico aunque hay un modelo medianamente entrenado con lenguaje juríco que es Roberta, pero este embeddings no esta listo para tareas semánticas e iba a tener que hacerlo manualmente"
      ],
      "metadata": {
        "id": "I7sUW6C5Qy2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de librerias\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.util import cos_sim"
      ],
      "metadata": {
        "id": "7T1EejpVPhYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Carga de modelo y codificación de preguntas"
      ],
      "metadata": {
        "id": "i-IVQm0JSNkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo\n",
        "modelo_embedding = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Codificar preguntas del dataset (evitar nulos)\n",
        "preguntas_corpus = df['Pregunta'].fillna('').tolist()\n",
        "\n",
        "# Codificar todas las preguntas del corpus en vectores numéricos\n",
        "embeddings_corpus = modelo_embedding.encode(preguntas_corpus, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "JMK7zZTfQslB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Chatbot con embeddings MiniLm"
      ],
      "metadata": {
        "id": "J2TdcWy-SYB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_responder_minilm(pregunta_usuario):\n",
        "    entrada = pregunta_usuario.lower().strip()\n",
        "\n",
        "    # Detectar saludo básico\n",
        "    if entrada in ['hola', 'buenas', 'buen día', 'buenas tardes', 'buenas noches', 'qué tal', 'saludos']:\n",
        "        return \"Hola! ¿En qué puedo ayudarte?\"\n",
        "\n",
        "    # Codificar la pregunta con embeddings MiniLM\n",
        "    pregunta_emb = modelo_embedding.encode(pregunta_usuario, convert_to_tensor=True)\n",
        "    similitudes = cos_sim(pregunta_emb, embeddings_corpus)[0]\n",
        "\n",
        "    # Mejor coincidencia\n",
        "    idx_max = similitudes.argmax().item()\n",
        "    score_max = similitudes[idx_max].item()\n",
        "\n",
        "    # Evaluar umbral de similitud\n",
        "    if score_max > 0.5:\n",
        "        return df.iloc[idx_max]['Respuesta']\n",
        "    else:\n",
        "        return \"Lo siento, no encuentro una respuesta clara. ¿Querés reformular la pregunta?\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aB-f7qCCSGGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Evaluación similitud del coseno"
      ],
      "metadata": {
        "id": "MJtzMbynfTsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluar_similitud_minilm(pregunta_usuario):\n",
        "    # Codificar la pregunta como embedding con MiniLM\n",
        "    pregunta_emb = modelo_embedding.encode(pregunta_usuario, convert_to_tensor=True)\n",
        "\n",
        "    # Calcular similitud del coseno con todo el corpus embebido\n",
        "    similitudes = cos_sim(pregunta_emb, embeddings_corpus)[0].cpu().numpy()\n",
        "\n",
        "    # Obtener índice y puntaje de la mejor coincidencia\n",
        "    idx_max = similitudes.argmax()\n",
        "    score_max = similitudes[idx_max]\n",
        "\n",
        "    # Respuesta asociada\n",
        "    respuesta = df.iloc[idx_max]['Respuesta']\n",
        "\n",
        "    # Imprimir detalle\n",
        "    print(f\"Pregunta original:    {pregunta_usuario}\")\n",
        "    print(f\"Similitud máxima:     {score_max:.4f}\")\n",
        "    print(f\"Índice en dataset:    {idx_max}\")\n",
        "    print(f\"Pregunta coincidente: {df.iloc[idx_max]['Pregunta']}\")\n",
        "    print(f\"Respuesta sugerida:   {respuesta}\")\n",
        "\n",
        "    return score_max, idx_max, respuesta\n"
      ],
      "metadata": {
        "id": "ZoiFpnJ6fZma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se prueba la similitud\n",
        "pregunta = \"Delito doloso\"\n",
        "print(evaluar_similitud(pregunta))"
      ],
      "metadata": {
        "id": "ww_aY-ovftgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Se observa una similitud bastante baja del 99%, lo que sugiere que el embeddings encuentra una buena similitud con respecto a la pregunta usada por el usuario y la contenida en el dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "IlCZyehGUHsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Muestra ambos chatbots funcionando"
      ],
      "metadata": {
        "id": "kGxapUlqWK3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Lista de preguntas reformuladas para ver la similitud"
      ],
      "metadata": {
        "id": "jer5SyAyWt7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preguntas_test = [\n",
        "    \"¿Cómo se define el Derecho Constitucional?\",\n",
        "    \"¿Para qué sirve la Constitución Nacional?\",\n",
        "    \"¿Quién se encarga de supervisar si una ley es constitucional?\",\n",
        "    \"¿Qué tipo de derechos están garantizados en la Constitución?\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "zgrLQ9hRWPm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Prueba del Chatbot con TF-IDF"
      ],
      "metadata": {
        "id": "OOwONPGyEtuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    entrada = input(\"Tú: \")\n",
        "    if entrada.lower() in ['salir', 'exit', 'adiós', 'adios', 'chau', 'chao', 'nos vemos', 'gracias']:\n",
        "        print(\"\\nChatbot: ¡Hasta luego!\")\n",
        "        break\n",
        "\n",
        "    respuesta = chatbot_responder(entrada)\n",
        "    print(f\"\\nChatbot: {respuesta}\\n\")"
      ],
      "metadata": {
        "id": "-FvWg5mWZ1o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 Prueba del Chatbot con Embeddings"
      ],
      "metadata": {
        "id": "fUxHB9sca8LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    entrada = input(\"Tú: \")\n",
        "    if entrada.lower() in ['salir', 'exit', 'adiós', 'adios', 'chau', 'chao', 'nos vemos', 'gracias']:\n",
        "        print(\"\\nChatbot: ¡Hasta luego!\")\n",
        "        break\n",
        "\n",
        "    respuesta = chatbot_responder_minilm(entrada)\n",
        "    print(f\"\\nChatbot: {respuesta}\\n\")"
      ],
      "metadata": {
        "id": "axBkhYMCbFbK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}